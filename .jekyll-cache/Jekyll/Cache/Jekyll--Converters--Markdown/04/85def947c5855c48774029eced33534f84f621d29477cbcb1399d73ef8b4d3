I"˘é<ul>
  <li>
    <p><em>This post is best suited for people who are familiar with linear classifiers. I will also be assuming that the reader is familiar with gradient descent.</em></p>
  </li>
  <li>
    <p><em>The goal of this post isn‚Äôt to be a comprehensive guide about neural networks, but rather an attempt to show an intuitive path going from linear classifiers to a simple neural network.</em></p>
  </li>
</ul>

<p>There are many types of neural networks, each having some advantage over others. In this post, I want to introduce the simplest form of a neural network, a Multilayer Perceptron (MLP). MLPs are a powerful method for approximating functions and it‚Äôs a relatively simple model to implement (kinda‚Äô).</p>

<p>Before we jump into talking about MLPs, let‚Äôs quickly go over linear classifiers. Given training data as pairs <script type="math/tex">(\boldsymbol{x}_i, y_i)</script> where <script type="math/tex">\boldsymbol{x}_i \in \mathbb{R}^{d}</script> are datapoints (observations) and <script type="math/tex">y_i \in \{0, 1\}</script> are their corresponding class labels. The goal is to learn a vector of weights <script type="math/tex">\boldsymbol{w} \in \mathbb{R}^{d}</script> and a bias <script type="math/tex">b \in \mathbb{R}</script> such that <script type="math/tex">\boldsymbol{w}^T\boldsymbol{x}_{i} + b \ge 0</script> if <script type="math/tex">y_{i} = 1</script> and <script type="math/tex">% <![CDATA[
\boldsymbol{w}^T\boldsymbol{x}_{i} + b < 0 %]]></script> otherwise (<script type="math/tex">y_{i} = 0</script>). This decision can be summarized as the following step function:</p>

<script type="math/tex; mode=display">% <![CDATA[
\text{Prediction} = \begin{cases}
      1 & \boldsymbol{w}^T\boldsymbol{x} + b \ge 0 \\
      0 &  \text{Otherwise}\\
\end{cases} %]]></script>

<p>In the case of Logistic Regression the decision function is characterized by the sigmoid function <script type="math/tex">\sigma(z) = \frac{1}{1+e^{-z}}</script> where <script type="math/tex">z = \boldsymbol{w}^T\boldsymbol{x} + b</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\text{Prediction} = \begin{cases}
      1 & \sigma(z) \ge \theta \\
      0 &  \text{Otherwise}\\
\end{cases} %]]></script>

<p>Where <script type="math/tex">\theta</script> is a threshold that is usually set to be 0.5.</p>

<!-- *Note: These are actually just a couple of examples of a zoo of functions that people in deep learning literature refer to as activation functions.* -->

<p>If the dataset is linearly separable this is all fine, since we can always learn <script type="math/tex">\boldsymbol{w}</script> and <script type="math/tex">b</script> that separates the data perfectly. We‚Äôre in good shape even if the dataset is almost linearly separable, i.e the data points can be separated with a line, barring a few noisy observations.</p>

<p><img src="../images/blobs.png" alt="" /></p>

<p>But what can we do if the dataset is highly non-linear? For example something like this:</p>

<p><img src="../images/circles.png" alt="" /></p>

<p>One thing we could potentially do is to come up with some non-linear transformation function <script type="math/tex">\phi(\boldsymbol{x})</script>, such that applying it, renders the data linearly separable. Having this transformation function would allow us to use all the tools we have for linear classification.</p>

<!-- We can then apply that transformation to the original dataset and learn a linear classifier on the transformed dataset. -->

<p>For example in this case we can see that the data points come from two concentric circles. Using this information we define the following transformation function: <script type="math/tex">\phi(\boldsymbol{x}) = [x_1^2, x_2^2]</script></p>

<p>Now we can learn a vector <script type="math/tex">\boldsymbol{w}</script> and bias <script type="math/tex">b</script> such that <script type="math/tex">\boldsymbol{w}^T\phi(\boldsymbol{x}_{i}) + b \ge 0</script> if <script type="math/tex">y_{i} = 1</script> and <script type="math/tex">% <![CDATA[
\boldsymbol{w}^T\phi(\boldsymbol{x}_{i}) + b < 0 %]]></script> otherwise.</p>

<p><img src="../images/circles_transformed_clf.png" alt="" /></p>

<p>This works for this particular case because we know exactly what the data generation process is, But what can we do when it‚Äôs not obvious what the underlying function is? What if we‚Äôre working in high dimensions where we can‚Äôt visualize the shape of the dataset? In general it‚Äôs hard to come up with these transformation functions.</p>

<p>Here‚Äôs another idea, instead of learning one linear classifier, let‚Äôs try to learn three linear classifiers and then combine them to get something like this:</p>

<p><img src="../images/circles3.png" alt="" /></p>

<p>We know how to learn a single linear classifier but how can we learn three linear classifiers that can produce a result like this? The naive approach would be to try to learn them independently using different random initializations and hope that they converge to something like what we want. But this approach is doomed from the beginning since each classifier will try to fit the whole data while ignoring what the other classifiers are doing. In other words there will be no cooperation since none of the classifiers will be ‚Äúaware‚Äù of each other. This is the opposite of what we want. We want/need the classifiers to work together.</p>

<p>This is where MLPs come in. A simple MLP can actually do both of the aforementioned things. 1) It can learn a non-linear transformation that makes the dataset linearly separable and 2) it can learn multiple linear classifiers that cooperate.</p>

<p>The goal for the next section is to come up with a classifier that can potentially learn how to correctly classify the two concentric circles dataset.</p>

<!-- **Neural Networks:**

By far the most common way of introducing neural networks is with the notion of computational graphs. While I do think that computational graphs are an important concept to understand, I do not think that they are the best way to be introduced to neural networks. Instead I will be using concepts that hopefully you the reader are familiar with. These are the essential operations for neural networks: matrix multiplication, non-linear activation functions and function composition. -->

<!-- In general it's better to teach new ideas using concepts and terms that a person is already familiar with, since this allows for the already known things to function as a foundation to be built on, rather than trying build from scratch.

The term 'neural networks' itself is kind of misleading. It creates an image of a brain like structure and it feeds into the whole hype about AI taking over. Neural networks in reality are nothing but a chain of matrix multiplications followed by non-linear functions.   -->

<h2 id="design">Design</h2>

<h4 id="three-linear-classifiers">Three Linear Classifiers</h4>

<p>Let‚Äôs continue our idea of learning multiple linear classifiers. Define three classifiers <script type="math/tex">(\boldsymbol{w}_{1}, b_1), (\boldsymbol{w}_{2}, b_3)</script> and <script type="math/tex">(\boldsymbol{w}_{3}, b_3)</script>, where <script type="math/tex">\boldsymbol{w}_i \in \mathbb{R}^2</script> and <script type="math/tex">b_i \in \mathbb{R}</script>.</p>

<p>Because we want to learn all three jointly, it makes sense to combine them into a single object. Let‚Äôs stack all of the classifiers into a single matrix <script type="math/tex">\boldsymbol{W}^{3 \times 2}</script> and the biases into a vector <script type="math/tex">\boldsymbol{b}^{3 \times 1}</script>, as such:</p>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{W} = \begin{bmatrix}
           \boldsymbol{w}^T_{1} \\
           \boldsymbol{w}^T_{2} \\
           \boldsymbol{w}^T_{3} \\
         \end{bmatrix} = \begin{bmatrix}
                    w_1^{(1)} & w_1^{(2)}\\
                    w_2^{(1)} & w_2^{(2)}\\
                    w_3^{(1)} & w_3^{(2)}\\
                  \end{bmatrix} \boldsymbol{b} = \begin{bmatrix}
                             b_1 \\
                             b_2 \\
                             b_3 \\
                           \end{bmatrix} %]]></script>

<p>Now we need to get a classification decision from each one of the classifiers. We mentioned two types of decision functions in the beginning of the post, the step function and the sigmoid, which is basically a smooth step function. For technical reasons that will become clear in the next section, we‚Äôre gonna use the sigmoid function to produce decisions. For each pair <script type="math/tex">(\boldsymbol{w}_{i}, b_i)</script>, to get the prediction for a given data point we take <script type="math/tex">\sigma(\boldsymbol{w}_{i}^T\boldsymbol{x} + b_i)</script>. But this is not taking the advantage of having everything packed into a matrix. Instead of enumerating the classifiers one by one we could do everything in one operation.</p>

<script type="math/tex; mode=display">\sigma(\boldsymbol{Wx} + \boldsymbol{b}) = \begin{bmatrix}
\sigma(\boldsymbol{w}_{1}\boldsymbol{x} + b_1) \\
\sigma(\boldsymbol{w}_{2}\boldsymbol{x} + b_2) \\
\sigma(\boldsymbol{w}{3}\boldsymbol{x} + b_3) \\
\end{bmatrix}</script>

<p><em>Note: We are slightly abusing the notation here, but just to be clear, <script type="math/tex">\sigma</script> function for vector valued functions is an element-wise operation.</em></p>

<p>This is great, but so far we haven‚Äôt really done anything. We just came up with a neat way to compute the output of all three classifiers given some input. We still need to connect them in order to create ‚Äúcooperation‚Äù.</p>

<h4 id="the-meta-classifier">The Meta Classifier</h4>

<p>Let‚Äôs define another linear classifier, but this time instead of taking the data points as input, this classifier will take the outputs of the three classifiers as input and will output a final classification decision. In a way it‚Äôs a meta classifier, since it classifies using outputs of other classifiers.</p>

<p>Let <script type="math/tex">\boldsymbol{h}^{3 \times 1}</script> be the output of the previous classifiers, i.e <script type="math/tex">\boldsymbol{h} = \sigma(\boldsymbol{Wx} + \boldsymbol{b})</script>, then the prediction of the meta classifier <script type="math/tex">(\boldsymbol{w}_{m}, b_{m})</script> is defined as: <script type="math/tex">\sigma(\boldsymbol{w}_{m}^T\boldsymbol{h} + b_{m})</script>, where <script type="math/tex">\boldsymbol{w}_{m} \in \mathbb{R}^3</script> and <script type="math/tex">b_{m} \in \mathbb{R}</script>.</p>

<!-- This classifier will have 2 jobs: 1) combine outputs in the forward pass and 2) act as a "conductor" by ensuring coordination during the backward pass. -->
<!--
This classifier which is kind of a meta classifier, will essentially act as a conductor, making sure there is coordination between the previous three classifiers. -->

<p>And there it is, we finally have everything. All three classifiers are connected, we have a way to produce a single prediction using all three of them and there is hope that coordination will happen because of the meta classifier.</p>

<p>Just to recap, the expression bellow is the function that corresponds to our MLP:</p>

<script type="math/tex; mode=display">\text{MLP}(\boldsymbol{x}; \boldsymbol{w}_{m}, b_{m}, \boldsymbol{W}, \boldsymbol{b}) =\sigma(\boldsymbol{w}_{m}^T\sigma(\boldsymbol{Wx} + \boldsymbol{b}) + b_{m})</script>

<p>Everything before the semicolon is the input of the function and everything after are the parameters of the function. Our goal is to learn the parameters.</p>

<p><strong>Exercise:</strong>
A question that you might have at this point is ‚Äúwhy do we need to have a decision function applied to the three linear classifiers, can‚Äôt we directly plug the outputs to the meta classifier and produce a decision? ‚Äú. I‚Äôm gonna leave the answer to that as an exercise. Remove all the <script type="math/tex">\sigma</script> functions, and simplify the expression. What do you get? Is it different than having a single linear classifier?</p>

<h2 id="learn">Learn</h2>

<p>We have managed to define a simple MLP but we still need a way to learn the parameters of the function. The function is fully differentiable and this is no accident. As I said earlier, we chose to use the sigmoid function instead of the step-function as a decision function because of technical reasons. Well the technical reason is that, differentiability is nice and we like it because it allows us to use gradient based optimization algorithms like gradient descent.</p>

<h4 id="loss-function">Loss Function</h4>

<p>Since the function is differentiable, we can define a loss function and then start optimizing with respect to the learnable parameters using gradient descent. Notice that the output of the MLP is a real number between 0 and 1. What we‚Äôre essentially doing is modeling the conditional distribution
<script type="math/tex">P(y | \boldsymbol{x})</script> with a parametrized function <script type="math/tex">MLP(\boldsymbol{x}; \theta)</script> <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>. This means that we can use the principle of maximum likelihood to estimate the parameters.</p>

<script type="math/tex; mode=display">L(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^n -y_i\log\hat{y_i} - (1-y_i)\log(1-\hat{y_i})</script>

<p>Where <script type="math/tex">\hat{y} = MLP(\boldsymbol{x}; \theta)</script>. The objective is to minimize <script type="math/tex">L(y, \hat{y})</script> <sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> with respect to the learnable parameters <script type="math/tex">\theta</script>.</p>

<h4 id="optimization">Optimization</h4>

<p>The plan is to use gradient descent to optimize <script type="math/tex">L</script>. Remember that during gradient descent, we need take the gradient (hence the name) of the objective at every step of the algorithm.</p>

<script type="math/tex; mode=display">\theta \leftarrow \theta - \alpha \nabla_{\theta} L</script>

<p>Where <script type="math/tex">\alpha</script> is the step size (learning rate).</p>

<p>Since <script type="math/tex">L</script> is a composition function, we will need to use the chain rule (Remember Calc 1). Furthermore, <script type="math/tex">\theta</script> isn‚Äôt a single variable, we will be optimizing with respect to 4 different variables <script type="math/tex">\boldsymbol{w}_{m}, b_{m}, \boldsymbol{W}, \boldsymbol{b}</script>. We‚Äôre going to need to update each one at every step:</p>

<p><script type="math/tex">\boldsymbol{w}_{m} \leftarrow \boldsymbol{w}_{m} - \alpha * \frac{\partial L}{\partial \boldsymbol{w}_{m}}</script> <br />
  <script type="math/tex">b_{m} \leftarrow b_{m} - \alpha * \frac{\partial L}{\partial b_{m}}</script> <br />
  <script type="math/tex">\boldsymbol{W} \leftarrow \boldsymbol{W} - \alpha * \frac{\partial L}{\partial \boldsymbol{W}}</script> <br />
  <script type="math/tex">\boldsymbol{b} \leftarrow \boldsymbol{b} - \alpha * \frac{\partial L}{\partial \boldsymbol{b}}</script></p>

<h4 id="derivatives-derivatives-derivatives">Derivatives, Derivatives, Derivatives</h4>

<p><em>Skip this section if you don‚Äôt care about all of the gory details of computing the partials. Although I do think that it‚Äôs a good idea to do this at least once by hand.</em></p>

<p>Now we will need to breakdown each of the partial derivatives using the chain rule. If we‚Äôre not careful about giving names to intermediate values, it will quickly get hairy. So let‚Äôs do that first.</p>

<p><script type="math/tex">\boldsymbol{s}_1 = \boldsymbol{Wx} + \boldsymbol{b}</script> <br />
  <script type="math/tex">\boldsymbol{h} = \sigma(\boldsymbol{s}_1)</script> <br />
  <script type="math/tex">s_2 = \boldsymbol{w}^T_{m}\boldsymbol{h} + b_{m}</script> <br />
  <script type="math/tex">\hat{y} = \sigma(s_2)</script></p>

<p>Before we start the tedious process of taking partial derivatives of a composed function I want to remind you that the goal is to compute these four partial derivatives: <script type="math/tex">\frac{\partial L}{\partial \boldsymbol{w}_{m}}, \frac{\partial L}{\partial b_{m}}, \frac{\partial L}{\partial \boldsymbol{W}}, \frac{\partial L}{\partial \boldsymbol{b}}</script>. If we have these values, we can use them to update the parameters at each step of gradient descent. Using the chain rule we can write down each of the partial derivatives as a product:</p>

<p><script type="math/tex">\frac{\partial L}{\partial \boldsymbol{w}_{m}} = \frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial s_2}\frac{\partial s_2}{\partial \boldsymbol{w}_{m}}</script> <br />
  <script type="math/tex">\frac{\partial L}{\partial b_{m}} = \frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial s_2}\frac{\partial s_2}{\partial b_{m}}</script> <br />
  <script type="math/tex">\frac{\partial L}{\partial \boldsymbol{W}} = \frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial s_2}\frac{\partial s_2}{\partial \boldsymbol{h}}\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{s}_1}\frac{\partial \boldsymbol{s}_1}{\partial \boldsymbol{W}}</script> <br />
  <script type="math/tex">\frac{\partial L}{\partial \boldsymbol{b}} = \frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial s_2}\frac{\partial s_2}{\partial \boldsymbol{h}}\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{s}_1}\frac{\partial \boldsymbol{s}_1}{\partial \boldsymbol{b}}</script></p>

<p>I know this looks complex but it really isn‚Äôt that complicated. All we‚Äôre doing is taking a partial derivative of the loss with respect to each of the learnable parameters. Since the loss is a composition function we have to use chain rule. That‚Äôs it.</p>

<p>We can see that <script type="math/tex">\frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial s_2}</script> is shared among all of them and that <script type="math/tex">L, \hat{y}, s_2</script> are all scalar variables therefore the derivatives are relatively easy to compute.</p>

<p><script type="math/tex">\frac{\partial L}{\partial \hat{y}} = \frac{\hat{y}-y}{\hat{y}(1-\hat{y})}</script> <br />
  <script type="math/tex">\frac{\partial \hat{y}}{\partial s_2} = \hat{y}(1-\hat{y})</script> (Recall that <script type="math/tex">\sigma^{'}(z) = (1-\sigma(z))\sigma(z)</script>)</p>

<p>Hence <script type="math/tex">\frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial s_2} = \hat{y}-y</script>.</p>

<p>Continuing down the chain we get:</p>

<p><script type="math/tex">\frac{\partial s_2}{\partial \boldsymbol{w}_{m}} = \boldsymbol{h}</script> <br />
  <script type="math/tex">\frac{\partial s_2}{\partial b_{m}} = 1</script> <br />
  <script type="math/tex">\frac{\partial s_2}{\partial \boldsymbol{h}} = \boldsymbol{w}_{m}</script></p>

<p>Now since, <script type="math/tex">\boldsymbol{h}</script> and <script type="math/tex">\boldsymbol{s_1}</script> are both vectors, the partial <script type="math/tex">\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{s_1}}</script> will be a matrix, however it will be a diagonal matrix.</p>

<script type="math/tex; mode=display">\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{s_1}} = \text{diag}((\boldsymbol{1} - \boldsymbol{h}) \odot \boldsymbol{h})</script>

<p>Which can be replaced by an element-wise multiplication in the chain as: <script type="math/tex">\odot (\boldsymbol{1} - \boldsymbol{h}) \odot \boldsymbol{h}</script></p>

<p>The partial derivative <script type="math/tex">\frac{\partial \boldsymbol{s_1}}{\partial \boldsymbol{W}}</script> is the most complicated to compute. <script type="math/tex">\boldsymbol{s_1}</script> is a vector and <script type="math/tex">\boldsymbol{W}</script> is a matrix, therefore the result of the partial derivative will be a 3 dimensional tensor! But fortunately, we will be able to reduce it to something more simple.</p>

<p>Instead of computing the partial derivative with respect to entire weight matrix, let‚Äôs instead take derivatives with respect to each of the classifiers <script type="math/tex">\boldsymbol{w_1}, \boldsymbol{w_2},</script> and <script type="math/tex">\boldsymbol{w_3}</script> (These would correspond to the rows of <script type="math/tex">\boldsymbol{W}</script>). Each of these derivatives will be a matrix instead of a tensor.</p>

<p><script type="math/tex">% <![CDATA[
\frac{\partial \boldsymbol{s_1}}{\partial \boldsymbol{w_1}} = \begin{bmatrix}
             x_1 && x_2\\
             0 && 0 \\
             0 && 0 \\
           \end{bmatrix} %]]></script> <br />
  <script type="math/tex">% <![CDATA[
\frac{\partial \boldsymbol{s_1}}{\partial \boldsymbol{w_2}} = \begin{bmatrix}
            0 && 0\\
            x_1 && x_2 \\
            0 && 0 \\
          \end{bmatrix} %]]></script> <br />
  <script type="math/tex">% <![CDATA[
\frac{\partial \boldsymbol{s_1}}{\partial \boldsymbol{w_3}} = \begin{bmatrix}
            0 && 0\\
            0 && 0 \\
            x_1 && x_2 \\
          \end{bmatrix} %]]></script></p>

<p>We know that we‚Äôre gonna be using these values in a multiplication. We can use this fact to simplify the expression for the derivative. Let <script type="math/tex">\frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial s_2}\frac{\partial s_2}{\partial \boldsymbol{h}}\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{s}_1} = \boldsymbol{\delta}</script>, then we‚Äôll have</p>

<p><script type="math/tex">\boldsymbol{\delta} \frac{\partial \boldsymbol{s_1}}{\partial \boldsymbol{w_1}} = [\delta_1x_1, \delta_1x_2]</script> <br />
  <script type="math/tex">\boldsymbol{\delta} \frac{\partial \boldsymbol{s_1}}{\partial \boldsymbol{w_2}} = [\delta_2x_1, \delta_2x_2]</script> <br />
  <script type="math/tex">\boldsymbol{\delta} \frac{\partial \boldsymbol{s_1}}{\partial \boldsymbol{w_3}} = [\delta_3x_1, \delta_3x_2]</script></p>

<p>Which implies that <script type="math/tex">% <![CDATA[
\boldsymbol{\delta}\frac{\partial \boldsymbol{s_1}}{\partial \boldsymbol{W}} = \begin{bmatrix}
          \delta_1x_1 && \delta_1x_2\\
          \delta_2x_1 && \delta_2x_2 \\
          \delta_3x_1 && \delta_3x_2 \\
        \end{bmatrix} %]]></script></p>

<p>We can rewrite this compactly as an <em>outer product</em> between <script type="math/tex">\boldsymbol{\delta}</script> and <script type="math/tex">\boldsymbol{x}</script>.</p>

<script type="math/tex; mode=display">\frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial s_2}\frac{\partial s_2}{\partial \boldsymbol{h}}\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{s}_1}\frac{\partial \boldsymbol{s_1}}{\partial \boldsymbol{W}} = \boldsymbol{\delta} \otimes \boldsymbol{x}</script>

<p>And finally</p>

<script type="math/tex; mode=display">\frac{\partial \boldsymbol{s_1}}{\partial \boldsymbol{b}} = \text{diag}(\boldsymbol{1}) = \boldsymbol{I}</script>

<p>Putting everything together:</p>

<p><script type="math/tex">\frac{\partial L}{\partial \boldsymbol{w}_{m}} = (\hat{y} - y)\boldsymbol{h}</script> <br />
  <script type="math/tex">\frac{\partial L}{\partial b_{m}} = \hat{y} - y</script> <br />
  <script type="math/tex">\frac{\partial L}{\partial \boldsymbol{W}} = ((\hat{y} - y)\boldsymbol{w}_{m}\odot (\boldsymbol{1} - \boldsymbol{h}) \odot \boldsymbol{h}) \otimes \boldsymbol{x}</script> <br />
  <script type="math/tex">\frac{\partial L}{\partial \boldsymbol{b}} = ((\hat{y} - y)\boldsymbol{w}_{m}\odot (\boldsymbol{1} - \boldsymbol{h}) \odot \boldsymbol{h})^T</script></p>

<p>You may have noticed that all of this is for a single datapoint <script type="math/tex">\boldsymbol{x}</script>, we wouldn‚Äôt do this in practice. It is much more preferable to have everything computed for a batch of inputs <script type="math/tex">\boldsymbol{X}</script>, this allows us to update the parameters much more efficiently. I highly recommend you redo all of the computations of the partial derivatives in matrix form.</p>

<p>I‚Äôve also purposefully skipped over a lot of the details. I wanted this block of the post to serve as a reference for your own solutions rather than a complete step-by-step guide. Here are some useful notes that can come in handy if you want to do everything from scratch:</p>

<ul>
  <li><a href="http://cs231n.stanford.edu/vecDerivs.pdf">Vector, Matrix, and Tensor Derivatives - Erik Learned-Miller</a></li>
  <li><a href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf">Computing Neural Network Gradients - Kevin Clark</a></li>
</ul>

<h2 id="results">Results</h2>
<p>Phew! now that‚Äôs over with. Let‚Äôs see what the results after running gradient descent (1000 iterations with a learning rate of 0.01). Do you remember how we started? We said that if only we had a transformation function that could make the dataset linearly separable then learning would be easy. Well <script type="math/tex">\phi(\boldsymbol{x}) = \sigma(\boldsymbol{Wx} + \boldsymbol{b})</script> will actually be that transformation that makes the dataset linearly separable. This is what the data looks like after applying that learned function:</p>

<p><img src="../images/projection.png" alt="" /></p>

<p>As you can see the data is completely linearly separable. In essence, this is what most of learning is when it comes to neural networks. Every neural network classifier that has classification as a primary task is trying to learn some kind of a transformation on the data so that the data becomes linearly separable. This is a big reason why neural networks became so popular. In the past people (usually domain experts) spent tremendous efforts in engineering features to make learning easy. Now a lot of that is handled by (deep) neural networks <sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>.</p>

<p>We were also trying to learn multiple linear classifiers. and voil√†, these are the three linear classifiers <script type="math/tex">(\boldsymbol{w}_{1}, b_1), (\boldsymbol{w}_{2}, b_3)</script> and <script type="math/tex">(\boldsymbol{w}_{3}, b_3)</script> that are learned:</p>

<p><img src="../images/hidden_classifiers.png" alt="" /></p>

<p>And finally this is what the learned decision boundary looks like in the original space. The colors indicate the predictions of the classifier.</p>

<p><img src="../images/decision_boundary.png" alt="" /></p>

<p>This is awesome isn‚Äôt it? but wait hold on. While this classifier gets 100% accuracy, it does not represent the true function‚Ä¶ With three classifiers the shape we are learning is a triangle-ish shape. That‚Äôs because it‚Äôs the only possible shape that captures all the data with three classifiers. But we know that the actual function is a circle. With four classifiers we can get rectangle-ish shapes, with five a pentagon-ish and so on. Intuitively, if we added a lot more classifiers we should get closer to an actual circle. Here‚Äôs a progression of the decision boundary going from 5 to 50 with increments of 5:</p>

<p><img src="../images/decision_boundaries_progress.png" alt="" /></p>

<p>This looks much better. But wait‚Ä¶ This isn‚Äôt really the true function either. Everything in the middle is classified as red, but there will never by any points there. The true function generates points on the boundary of the circle, never inside the circle. Furthermore, the only reason we were able to make this correction was because we‚Äôre working in 2 dimensions and we know exactly what the true function is. What do we do if we have a dataset in high dimensions coming from an unknown function? Would we be able to trust the learned classifier even if we get 100% accuracy?</p>

<h2 id="jargon">Jargon</h2>

<p>For the entirety of the post I have purposefully avoided mentioning neural network lingo that you usually see in the literature. I think some of the terms itself can bring a lot of confusion to people when they first get introduced to neural networks. However, since the field is set on using these terms, it‚Äôs necessary to know them. Let‚Äôs go back and put names on some of the things we‚Äôve talked about.</p>

<h3 id="activation-functions">Activation Functions</h3>

<p>We talked about decision functions. We mentioned the step function and the sigmoid function. The justification for having them was straight-forward since we were talking in the context of classifiers and we had to have a function that produces a prediction. But in the context of neural networks we don‚Äôt really care for predictions if it isn‚Äôt the last classifier (the meta-classifier). Every intermediate function can have any form, as long as it‚Äôs differentiable.</p>

<p>So we aren‚Äôt constrained to using functions that produce predictions like sigmoid or the step function. Here are a few others we could have used: Tanh, ReLu, LeakyReLu, SoftPlus etc. People refer to these functions as activation functions. The most popular choice in practice is the ReLu activation defined as <script type="math/tex">\text{ReLu}(z)=\max(0, z)</script>. Activation functions are usually non-linear which is the reason why neural networks are able to learn non-linear functions. When the input is a vector or a matrix, the activation function is applied element-wise.</p>

<h3 id="unit-neuron">Unit (Neuron)</h3>

<p>As we mentioned above, we don‚Äôt really need to predict in the intermediate operations. Therefore, we probably shouldn‚Äôt be calling these functions classifiers. People usually call these functions neurons or units. I prefer to call them units since calling to them neurons is drawing a parallel to biological neurons which are not similar at all. A unit takes the following form:</p>

<script type="math/tex; mode=display">g(\boldsymbol{w}^T\boldsymbol{x} + b) = y</script>

<p>Where <script type="math/tex">g</script> is some (usually non-linear) activation function.</p>

<h3 id="layer">Layer</h3>

<p>A layer in the context of an MLP is a linear transformation followed by an activation function. A bunch of neurons together on the same level make a layer. What a level means will be more clear when we see the graphical representation of neural networks.</p>

<p>In this post we defined a 2 layer MLP.</p>
<ul>
  <li>Layer 1: Linear transformation <script type="math/tex">\boldsymbol{Wx} + \boldsymbol{b} = \boldsymbol{s}_1 \rightarrow</script> activation <script type="math/tex">\rightarrow \sigma(\boldsymbol{s}_1) = \boldsymbol{h}</script></li>
  <li>Layer 2: Linear transformation <script type="math/tex">\boldsymbol{w}_{m}^T\boldsymbol{h} + b_{m} = \boldsymbol{s}_2 \rightarrow</script> activation <script type="math/tex">\sigma(\boldsymbol{s}_2) \rightarrow \hat{y}</script></li>
</ul>

<p>People refer to the layers before the last layer as hidden layers. In this case we only had one hidden layer (Layer 1).</p>

<p><strong>More layers</strong>
In practice we usually have many such layers each connected to each other, i.e the output of one becomes the input for to next one. Chaining layers like this is actually the same as function composition. If we define each layer as a function, <script type="math/tex">f_i(x) = g(\boldsymbol{W}_i\boldsymbol{x} + \boldsymbol{b}_i)</script>, where <script type="math/tex">g</script> is some activation function, then an n-layer MLP can be written as the function composition <script type="math/tex">MLP(x) = f_n(f_{n-1}(...(f_1(x)))</script>. The depth of a network corresponds to <script type="math/tex">n</script>. A network with depth <script type="math/tex">n > 2</script>, is called deep (this is where the term deep learning comes from). The width of a network corresponds to the number of units in each of the layer.</p>

<h3 id="graph">Graph</h3>

<p>You may have been confused about the fact that MLP is called a neural network. So far we haven‚Äôt seen the ‚Äúnetwork‚Äù part. The MLP that we defined can equivalently be represented by a directed acyclic graph (DAG).</p>

<p><img src="../images/nn.png" alt="" /></p>

<p>These kinds of graphs are called computational graphs and they are just another way to describe a neural network model. It provides a good way to break down a complex computation into its‚Äô primitive parts.</p>

<p>All of the edges correspond to the weights (parameters) of the model. The nodes represent computation. For example <script type="math/tex">h_1</script> represents the following computation:</p>

<script type="math/tex; mode=display">h_{1} = \sigma(\boldsymbol{w}_{1}^T\boldsymbol{x} + b_{1})</script>

<p>Edges coming out of the node that have a 1 on it are the biases.</p>

<p>To make sense of the rest of the edges, let‚Äôs highlight a path of a single unit  <script type="math/tex">(\boldsymbol{w_1}, b_{1})</script> to the output:</p>

<p><img src="../images/nn_single.png" alt="" /></p>

<p>This representation is useful for computing gradients. If we wanted to take the derivative of the loss with respect to the first unit, the highlighted path tells us that we have to start from the last output and work our way backwards until we reach the desired variables.</p>

<p>In this post we calculated all of the gradients by hand but in practice this is done through the algorithm known as backpropagation. It works by repeatedly applying the chain rule to compute all the gradients.</p>

<h3 id="forward-pass">Forward pass</h3>

<p>Running through the graph and computing all the values is called the forward pass. It‚Äôs called forward pass because we‚Äôre traveling from the first layer to the last.</p>

<h3 id="backward-pass">Backward pass</h3>

<p>Computing the derivatives of all the parameters with respect to the outputs is called a backward pass. Similar to forward pass, the backward pass is called backward because we‚Äôre traversing starting from the last layer and working our way back.</p>

<h2 id="final-words">Final Words</h2>

<p>I hope this post has provided some insight to you on how neural networks work. It is by no means comprehensive, I have skipped over a lot of really important details. If you want to continue learning about neural networks, the <a href="https://www.deeplearningbook.org/">Deep Learning book by Ian Goodfellow and Yoshua Bengio and Aaron Courville</a> is a good place to start. Here are a few other good resources:</p>

<ul>
  <li><a href="https://playground.tensorflow.org">Neural Network Playground</a> One of the best ways to learn something is to play around with it. The NN playground lets you easily build and train models and various synthetic datasets. Great tool for building intuition.</li>
  <li><a href="http://cs231n.github.io/">CS231n: Convolutional Neural Networks for Visual Recognition</a> Contains excellent notes from Andrej Karpathy, highly recommend</li>
  <li><a href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/">CSC 321: Intro to Neural Networks and Machine Learning</a> Has more than just neural networks. The lecture slides and notes are really good and it builds up from linear classifiers.</li>
  <li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">3Blue1Brown: Neural Networks</a> One of my all time favorite educational channels</li>
  <li><a href="http://cs229.stanford.edu/notes2019fall/cs229-notes-deep_learning.pdf">CS229 Lecture Notes by Andrew Ng: Deep Learning</a> Andrew Ng‚Äôs tutorials are always very good</li>
</ul>

<hr />
<h2 id="code">Code</h2>

<p>What‚Äôs a tutorial without code, am I right? Here‚Äôs a link to the Jupyter notebook that contains all the code for this post: <a href="https://github.com/colonialjelly/multilayer-perceptron/blob/master/multilayer-perceptron.ipynb">Code</a></p>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>To simplify the notation I‚Äôm referring to all of the parameters <script type="math/tex">\boldsymbol{w}_{m}, b_{m}, \boldsymbol{W}, \boldsymbol{b}</script> with just <script type="math/tex">\theta</script>.¬†<a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>What we have written here is the negative log-likelihood. Some people refer to this loss function as binary cross-entropy loss. These are equivalent loss functions, the only difference is the method/assumptions that one uses to arrive at each.¬†<a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>There are downsides to this, I‚Äôll write a post about this in the future (hopefully).¬†<a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET