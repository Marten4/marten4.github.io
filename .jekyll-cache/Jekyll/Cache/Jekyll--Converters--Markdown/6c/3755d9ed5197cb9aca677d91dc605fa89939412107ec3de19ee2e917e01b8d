I"p!<p>Every classification problem has some “true” set of features. These features, if given, renders the problem almost entirely solved. Unfortunately,  there is no way to directly uncover what these true features are. Instead we use a labeled training set to learn a mapping function from the inputs to the labels. The hope is that by learning the mapping, we would also learn at least some or most of these true features. However, the problem is that, the training set may also contain features that are not part of the true set but do help with the mapping. This most often happens because of artifacts in data collection. I’ll refer to these type of features as pseudo features.</p>

<p><strong>Definition:</strong> Pseudo features are features that have a high predictive power at training time but either do not provide any help or hurt with generalization.</p>

<p>To give an intuitive example, let’s say that we’re working on image classification and for some reason one of the classes has a bias in the training data. For example all of the pictures that have a bear on it were taken in a snowy background. Since the model is trying to find features that are correlated with the label it <em>may</em> learn that a lot of white pixels means that there’s a bear on the picture. Obviously, this is an oversimplification of the problem but hopefully you understand the point I’m trying to make.</p>

<center>
<tr>
<td> <img src="../images/snow_bear.jpg" alt="Bear" style="width: 300px" border="5" /> </td>
<td> <img src="../images/snow_no_bear.jpg" alt="No Bear" style="width: 300px" border="5" /> </td>
</tr>
</center>

<p>Unfortunately, these kinds of data biases are not rare and they are extremely difficult to catch. Rolling out models in the real-world with the intention of using them for critical decisions is scary if you don’t know for sure that the model is not using pseudo features for predictions.</p>

<p>One potential solution to this problem could be to expand the objective of training. For classification, we usually provide labeled training data and optimize the parameters of the model such that the outputs match the labels. This objective promotes features that help with prediction on the training set. But as we’ve discussed earlier, pseudo features can also be promoted during this process. The question is, how can we demoted these pseudo features at training time without knowing what they are? One thing we could do is to use a proxy</p>

<p>Let’s call these instances <script type="math/tex">D_{in}</script>, for every instance <script type="math/tex">x_i \in D_{in}</script>, we have a corresponding class label <script type="math/tex">y_i \in Y</script>. But we also have access to <script type="math/tex">D_{out}</script>, a set that contains items that are not part of <script type="math/tex">D_{in}</script>. For example if our task is 3 way document classification with the categories sports, politics and finance. The everything else could be documents about entertainment, fashion, high tech etc. Or we could expand the definition even more and include everything in the universe that is not <script type="math/tex">D_{in}</script>. We could use the instances in <script type="math/tex">D_{out}</script> to push the model to be uncertain on those examples. Which intuitively makes sense, if a model is trained to recognize hand-written digits and we show it a picture of the letter “A”, the desired output of the classifier should be maximum uncertainty over the output choices (digits from 0 to 9).</p>

<p>This objective has come up in recent years in the context of OOD detection <a href="#1">[1]</a><a href="#2">[2]</a>. They use <script type="math/tex">D_{out}</script> to supervise the model on detecting OOD samples. And they have shown this method to be successful at that. But I think there is potential for another outcome. Going back to our example with the bear, What if we expose the model to images that have a snowy background but do not have an object of our class in the image. We then push the model to be uncertain on those images. The model would (hopefully) learn that white pixels <script type="math/tex">\neq</script> bear and that it needs to ‘pick up’ other features to be able to classify the pictures with a bear on it. In other words, OOD data can potentially lead to a more robust solution by discarding the pseudo features. But we need to slightly change what we call <script type="math/tex">D_{out}</script>, it need not be OOD. Furthermore, not every OOD instance belongs to <script type="math/tex">D_{out}</script>. A more accurate description of the samples that we seek is out-of-concept (OOC) instances. The same concept (images of bears) can have different distributions depending on how the data was collected. We care about the concept of interest not being present in <script type="math/tex">D_{out}</script>.</p>

<p>Caveat: robustness is hard to measure if the test set also contains the pseudo features.</p>

<p>To check if this idea has any merit I designed a super simple experiment. The idea was to create the most ideal scenario. If this method doesn’t work for this, it wouldn’t work anything.</p>

<p>To start, I define a boolean function:</p>

<script type="math/tex; mode=display">(x_1 \land \neg x_2) \land (x_3 \lor x_4) \lor x_1 \lor (x_5 \land x_6) \land \neg x_7</script>

<p>The reason I’m using this function is because it has a big enough sample space <script type="math/tex">(2^7)</script> to make it possible to learn the function from a random sub-sample and the output space is roughly balanced (about half are zeros and half are ones). The function is also linear, so we can learn it with a linear function. To construct a dataset we create a truth table and label all of the outputs with the defined function. To verify that the dataset is learnable, I fit a linear classifier on 50% of the dataset and test it on the rest. The results show that the function is indeed recoverable from only half the data points. I ran 10 trials with different splits each time and get about ~0.945 accuracy on the test set with a standard deviation of ~0.03. This is the ceiling on the performance.</p>

<p>For the second part of the experiment I add a pseudo feature to the dataset. This feature is perfectly correlated with the label in the training set but is randomized in the test set. The train and test set are created by equally splitting the original dataset. I again fit a linear classifier on this modified dataset. I run 100 trials and report the mean accuracy and the standard deviation on both the train and the test set. As expected the results show that there is a perfect, 1.00 accuracy on the training set but the performance on the test set drops all the way to ~0.64 with a standard deviation of ~0.10. This is explained by the fact that the pseudo feature has a perfect correlation with the label in the training set but is randomized in the test set. At training time the model ends up relying on this feature too much, which causes it to suffer at test time. We can verify this by observing the magnitude of the coefficient corresponding to that feature.</p>

<p>Finally, I create a OOD dataset that is entirely zeros expect for the feature that corresponds to the pseudo feature. The modified loss function will contain a term that will prefer a low KL divergence on the output of the model on these examples and the uniform distribution. In other words, the objective will force the model to be uncertain on OOD samples.</p>

<p><strong>Loss:</strong>  <script type="math/tex">\mathbb{E}_{x \sim D_{in}} \text{L}(y, \bar{y}) + \lambda\mathbb{E}_{x \sim D_{out}}\text{KL}(U \| \bar{y})</script></p>

<p>The results after 100 random trials show that the new objective helps regularize the model by essentially canceling the pseudo feature. The test set accuracy is ~0.91 with a standard deviation of ~0.03. The accuracy isn’t as high is the model that does not get the pseudo feature at all, but it is definitely much higher than 0.64. Furthermore, the weight of the pseudo feature is consistently low, compared to the baseline model.</p>

<h2 id="references">References</h2>
<p><a id="1">[1]</a>
Lee et al.
<a href="https://arxiv.org/pdf/1711.09325.pdf">Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples.</a>
ICLR 2018.</p>

<p><a id="2">[2]</a>
Hendrycks et al.
<a href="https://arxiv.org/pdf/1812.04606.pdf">Deep Anomaly Detection with Outlier Exposure.</a>
ICLR 2019.</p>
:ET