I"R<p>About 6-8 months ago I was working on the problem of detecting out-of-distribution (OOD) samples. One of the papers I was following was using OOD samples in their loss in a clever way. They were minimizing the loss on the in distribution data (training data) and at the same time they were maximizing entropy (uncertainty) on out of distribution samples. This led to the model learning to recognize when it was shown something that does not look like the training data. For example you train a model on MNIST and show it a picture of a cat. It should throw up its’ hands and say this is not a digit. That’s exactly what the result was.</p>

<p>This approach made me think about the following question: Can out of distribution data actually help with in distribution generalization? Here’s why I think it would. Let’s say you’re working on image classification and one of the classes has a bias in the training data. For example all of the pictures that display a bear were taken in a snowy background. The model may learn that a lot of white pixels means that there’s a bear on the picture. This actually happens a lot, especially with neural nets [cite]. Now this is the question, what if we show the model a bunch of other pictures that do not belong to the in class. We also optimize it to be uncertain on those images. If at least some of the images in the out also have snowy background the model would (hopefully) lead to the model learning that white pixels do not necessarily mean bear and that it needs to ‘pick up’ other features to be able to classify the pictures with a bear on it.</p>
:ET