<!DOCTYPE html>
<html>
  <head>
    <title>Min Hashing – Giorgi – I think, therefore I'm distracted</title>
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Min Hashing | Giorgi</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Min Hashing" />
<meta name="author" content="Giorgi Kvernadze" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Suppose we want to cluster similar users on Spotify. With about 271 million users, if we assume that each user has about 1000 songs in their listening history we would need ~4.33 terabytes to represent the entire data. That is a lot! To make things even worse, in order to find the clusters with users, we need to measure the similarity between all pairs of users. With 271 million users that is an astronomical number of comparisons!" />
<meta property="og:description" content="Suppose we want to cluster similar users on Spotify. With about 271 million users, if we assume that each user has about 1000 songs in their listening history we would need ~4.33 terabytes to represent the entire data. That is a lot! To make things even worse, in order to find the clusters with users, we need to measure the similarity between all pairs of users. With 271 million users that is an astronomical number of comparisons!" />
<link rel="canonical" href="http://localhost:4000/minhashing/" />
<meta property="og:url" content="http://localhost:4000/minhashing/" />
<meta property="og:site_name" content="Giorgi" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-10T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2020-03-10T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/minhashing/"},"dateModified":"2020-03-10T00:00:00-06:00","author":{"@type":"Person","name":"Giorgi Kvernadze"},"description":"Suppose we want to cluster similar users on Spotify. With about 271 million users, if we assume that each user has about 1000 songs in their listening history we would need ~4.33 terabytes to represent the entire data. That is a lot! To make things even worse, in order to find the clusters with users, we need to measure the similarity between all pairs of users. With 271 million users that is an astronomical number of comparisons!","@type":"BlogPosting","url":"http://localhost:4000/minhashing/","headline":"Min Hashing","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="Suppose we want to cluster similar users on Spotify. With about 271 million users, if we assume that each user has about 1000 songs in their listening history we would need ~4.33 terabytes to represent the entire data. That is a lot! To make things even worse, in order to find the clusters with  users, we need to measure the similarity between all  pairs of users. With 271 million users that is an astronomical number of comparisons!

" />
    <meta property="og:description" content="Suppose we want to cluster similar users on Spotify. With about 271 million users, if we assume that each user has about 1000 songs in their listening history we would need ~4.33 terabytes to represent the entire data. That is a lot! To make things even worse, in order to find the clusters with  users, we need to measure the similarity between all  pairs of users. With 271 million users that is an astronomical number of comparisons!

" />
    
    <meta name="author" content="Giorgi" />

    
    <meta property="og:title" content="Min Hashing" />
    <meta property="twitter:title" content="Min Hashing" />
    

    
    <meta property="og:image" content="http://localhost:4000https://avatars0.githubusercontent.com/u/10067792?s=460&v=4"/>
    <meta property="twitter:image" content="http://localhost:4000https://avatars0.githubusercontent.com/u/10067792?s=460&v=4"/>
    
    <meta property="og:site_name" content="Amit Merchant - Software Engineer"/>


    <link rel="stylesheet" type="text/css" href="/assets/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Giorgi - I think, therefore I'm distracted" href="/feed.xml" />
    <link rel="canonical" href="http://localhost:4000/minhashing/" />

    <meta name="theme-color" content="#000000">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  </head>

  <body>
    <div id="bar"></div>
    <div class="wrapper-container">
      <div class="wrapper-masthead">
        <div class="container">
          <header class="masthead clearfix">
            <a href="/" class="site-avatar"><img src="https://avatars0.githubusercontent.com/u/10067792?s=460&v=4" /></a>

            <div class="site-info">
              <h1 class="site-name"><a href="/">Giorgi</a></h1>
              <p class="site-description">I think, therefore I'm distracted</p> 
            </div>

            <nav>
              <a href="/">Home</a>
              <a href="/reading">Reading</a>
              <a href="/search">Search</a>
              <a href="/about">About</a>
            </nav>
          </header>
        </div>
      </div>

      <div class="wrapper-main">
        <div id="main" role="main" class="container">
          <article class="post detailed">
  <h1>Min Hashing</h1>

  <div>
    <p class="author_title">Giorgi Kvernadze  ·  March 10, 2020</p>
    
    <div class="post-tags">
      
      
        <a href="/categories/#Data Mining">Data Mining</a>
        &nbsp;
      
        <a href="/categories/#Educational">Educational</a>
        
      
    </div>
  </div>
    
  <div class="entry">
    <p>Suppose we want to cluster similar users on Spotify. With about 271 million users, if we assume that each user has about 1000 songs in their listening history we would need ~4.33 terabytes to represent the entire data. That is a lot! To make things even worse, in order to find the clusters with <script type="math/tex">n</script> users, we need to measure the similarity between all <script type="math/tex">{n \choose 2} = \frac{n(n - 1)}{2}</script> pairs of users. With 271 million users that is an astronomical number of comparisons!</p>

<p>In this post, the goal will be to reduce the size of the representation of each user while preserving the similarities. This will take care of the memory issue and some of the computational burden. In the next post, we’ll talk about what to do with the bigger computational problem of calculating similarities between all of the pairs of users.</p>

<!--
Measuring similarity of objects is one of the most fundamental computations for data mining. Similarity can be used to detect plagiarism, categorize documents, recommend products to customers and there are many many more applications. There are a lot of different ways of defining similarity. In this post I'll be talking about Jaccard similarity and its' approximation. -->

<h3 id="toy-example">Toy example</h3>

<p>I think working with tiny examples to build intuition is an excellent method for learning. So in that spirit, let’s consider a toy example. Let’s assume that we only have 3 users and we have a total of 8 songs in our dataset.</p>

<p><img src="../images/user_data.png" alt="" /></p>

<h3 id="jaccard-similarity">Jaccard similarity</h3>

<p>The goal is to cluster similar users, so we obviously need a way to measure similarities between a pair of users. We will be using Jaccard similarity. It is defined as the fraction of shared elements between two sets. In our case the sets are users’ listening history. All we have to compute is how many songs each pair of users share with each other, divided by total number of songs in both users. For example the Jaccard similarity between <script type="math/tex">\text{user}_1</script> and <script type="math/tex">\text{user}_2</script></p>

<script type="math/tex; mode=display">J(\text{user}_1, \text{user}_2) = \frac{|\text{user}_1 \cap \text{user}_2|}{|\text{user}_1 \cup \text{user}_2|} = \frac{|\{1, 4, 7\}|}{|\{0, 1, 2, 4, 5, 7\}|} = \frac{3}{6} = 0.5</script>

<p>Similarly for the other pairs, we have:</p>

<center>
$$J(\text{user}_2, \text{user}_3) = \frac{3}{8} = 0.375$$
$$J(\text{user}_1, \text{user}_3) = \frac{0}{8} = 0$$
</center>

<p>An important fact about the Jaccard similarity is that it corresponds to the probability that a randomly selected element in the union is also in the intersection. This is a crucial property of the Jaccard similarity that is central to understanding why min hashing works.</p>

<h3 id="intuition-behind-the-jaccard-similarity">Intuition behind the Jaccard similarity</h3>

<p>For some people (present company included), visual explanations are easier to grasp than algebraic ones. We’ll briefly shift our view from sets to Venn diagrams. Let’s imagine any two users as Venn diagrams, the Jaccard similarity is the size of the intersection divided by the size of the union:</p>

<p><img src="../images/js_venn.png" width="50%" /></p>

<p>Now imagine that I’m throwing darts on the diagrams and I’m horrible at it. I’m so bad that every element on the diagrams has an equal chance of being hit. What’s the chance that I throw a dart and it lands on the intersection? It would be the number of elements in the intersection divided by the total number of elements. Which is exactly what the Jaccard similarity is. This implies that the larger the similarity, the higher the probability that we land on the intersection with a random throw.</p>

<p><img src="../images/venns.png" width="50%" /></p>

<p>Consider another scenario. Suppose you want to know the similarity between two sets, but you can’t see the diagram, you’re blindfolded. However, if you throw a dart, you do get the information on where it landed. Can you make a good guess on the similarity of two sets by randomly throwing darts on it? Let’s say after throwing 10 darts we know that 6 of them landed in the intersection. What would you guess that the similarity of the two sets are? Let’s say after throwing 40 more darts, we know that 30 of the total 50 throws landed in the intersection. What would your guess be now? Are you more certain about your guess? Why?</p>

<p>Ponder about this for a little bit and keep this picture in mind throughout reading this post. This is, in essence, the basis for the MinHash algorithm.</p>

<h3 id="approximate-jaccard-similarity">Approximate Jaccard similarity</h3>

<p>In the previous paragraph, we have alluded to the fact that it’s possible to approximate the Jaccard similarity between two sets. In order to see why that’s true, we need to rehash some of things we’ve said, mathematically.</p>

<p>Let’s take <script type="math/tex">\text{user}_1</script> and <script type="math/tex">\text{user}_2</script> and their union <script type="math/tex">\text{user}_1 \cup \text{user}_2 = \{0, 1, 2, 4, 5, 7\}</script>.
Some of the elements in union are also in the intersection, more specifically <script type="math/tex">\{1, 4, 7\}</script>.</p>

<p>Let’s replace the elements with the symbols “-“ and “+”, denoting if an element appears in the intersection or not.</p>

<script type="math/tex; mode=display">\{0, 1, 2, 4, 5, 7\} \rightarrow \{-, +, -, +, -, +\}</script>

<p>If every element has an equal probability of being picked, what is the probability of drawing an element that is of type “+”? It’s the number of pluses divided by number of pluses and number of minuses.</p>

<script type="math/tex; mode=display">P(\text{picking a "+"}) = \frac{\text{number of "+"}}{\text{number of "+" and "-"}}</script>

<p>The number of “+” corresponds to the number of elements in the intersection and the number of “+” and the number of “-“ corresponds to the total number of elements or the size of the union. Therefore,</p>

<script type="math/tex; mode=display">P(\text{picking a "+"}) = \frac{\text{number of "+"}}{\text{number of "+" and "-"}} = \frac{|\{1, 4, 7\}|}{|\{0, 1, 2, 4, 5, 7\}|} = J(\text{user}_1, \text{user}_2)</script>

<!-- More than two sets:

If we have more than two sets some things change. Now we can have multiple intersections. Just knowing that a dart landed in an intersection is not enough, we need to keep track of all possible intersections between the sets. This might be too much of a hassle. So we're gonna change the game.

Imagine that you're at a carnival and there's a shooting game. There are n diagrams on the board, you are again blindfolded. The game is to shoot for k rounds and guess the similarities between all the sets with **some** tolerance for error. Each round consists of up to n throws of a dart. Let's imagine that each throw has a guarantee to hit at least one diagram but it could potentially hit more than one (if it lands in an intersection). After a diagram gets hit, it gets eliminated. The round is over when all of the diagrams are eliminated. The diagrams get reset after each round. As before, you get to know where the dart lands, that is, you get to know the exact element that you hit and which of the diagrams were eliminated. Can you come up with a way to guess the similarities between all of the pairs of sets? -->

<!-- Let $$X$$ be a random variable such that $$X=1$$ if we draw a plus and $$X=0$$ if we draw a minus.

$$\mathbb{E}[X] = P(X=1) \times 1 + P(X=0) \times 0 = 0.5 = J(\text{user}_{1}, \text{user}_{2})$$ -->

<p>What this means is that we can approximate the Jaccard similarity between pairs of users. Let <script type="math/tex">X</script> be a random variable such that <script type="math/tex">X = 1</script> if we draw a plus and <script type="math/tex">X = 0</script> if we draw a minus. <script type="math/tex">X</script> is a Bernoulli random variable with <script type="math/tex">p=J(\text{user}_{1}, \text{user}_{2})</script>. In order to estimate the similarity we can estimate <script type="math/tex">p</script>. In this case we obviously know that <script type="math/tex">p=0.5</script> since we already computed it, but let’s assume that we don’t know this.</p>

<p>If we repeat the random draw multiple times and keep track of how many times a “+” type came up versus a “-“, we can estimate the parameter <script type="math/tex">p</script> for <script type="math/tex">X</script> by maximum likelihood estimation (MLE):</p>

<script type="math/tex; mode=display">\hat{p}  = \frac{1}{n} \sum_{i=1}^{n} X_{i} = \hat{J}(\text{user}_{1}, \text{user}_{2})</script>

<p>Where <script type="math/tex">X_{i}</script> are our observations and <script type="math/tex">n</script> is the total number of draws that were made. The larger the number of draws <script type="math/tex">n</script>, the better the estimation will be.</p>

<p>The code below will simulate the process a 30 times and empirically compute the similarity.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">num_trials</span> <span class="o">=</span> <span class="mi">30</span>

<span class="c1"># Union of user_1 and user_2
</span><span class="n">union</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>

<span class="c1"># Intersection of user_1 and user_2
</span><span class="n">intersection</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>

<span class="c1"># Randomly pick element
</span><span class="n">draws</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">union</span><span class="p">,</span> <span class="n">num_trials</span><span class="p">)</span>
<span class="n">num_intersect</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">draws</span><span class="p">,</span> <span class="n">intersection</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">num_intersect</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">draws</span><span class="p">))</span>

</code></pre></div></div>

<p>If you run the above code you should get something that is close to <script type="math/tex">0.5</script>. Which, as expected, corresponds to the Jaccard similarity between <script type="math/tex">\text{user}_1</script> and <script type="math/tex">\text{user}_2</script>. Play around with the variable <script type="math/tex">\text{num_trials}</script>, what happens if you set it to 1? What about 10,000?</p>

<!-- So what does this mean? This means that we can approximate Jaccard similarity using randomness. We're going to be using this fact to come up with a way to encode the original data into a smaller representation called a *signature* such that the Jaccard similarities are well approximated.  -->

<!-- This means that this random process in expectation is the same as the Jaccard similarity. We're going to be using this fact to come up with a way to encode the original data into a smaller representation such that the Jaccard similarities are well approximated. -->

<!-- Just to restate the goal, we have a dataset $$D$$ that we want to encode in some smaller dataset $$D^{'}$$ such that $$J_{pairwise}(D) \approx J_{pairwise}(D^{'})$$. Where $$J_{pairwise}$$ is the pairwise Jaccard similarity of all users in the data. -->

<!-- This is great, but we need to compute similarities between all pairs of users not just two users. -->

<h3 id="shuffling-and-picking-first-equiv-randomly-picking">Shuffling and picking first <script type="math/tex">\equiv</script> Randomly picking</h3>

<p>Before we move on, we need to understand one more thing. Randomly selecting an element from a set is the same thing as shuffling the set and picking the first element <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>. Everything that we have said above is also true if we, instead of randomly selecting an element, shuffled the set and picked the first element. Make sure to pause here, if this doesn’t make sense.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">num_trials</span> <span class="o">=</span> <span class="mi">30</span>

<span class="c1"># Union of user_1 and user_2
</span><span class="n">union</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>

<span class="c1"># Intersection of user_1 and user_2
</span><span class="n">intersection</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>

<span class="c1"># Shuffle and pick first element
</span><span class="n">num_intersect</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_trials</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">union</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">union</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">intersection</span><span class="p">:</span>
        <span class="n">num_intersect</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="n">num_intersect</span><span class="o">/</span><span class="n">num_trials</span><span class="p">)</span>
</code></pre></div></div>
<p>The code above implements the same process that I described before, but instead of randomly picking an element it is shuffling the elements in the union and picking the first element. If you run this you should similarly get something that is close to <script type="math/tex">0.5</script>.</p>

<h3 id="data-matrix">Data matrix</h3>

<p>We have shown that it’s possible to approximate Jaccard similarity for a pair of users using randomness. But our previous method had a significant issue. We still needed to have the intersection and the union set to estimate the Jaccard similarity, which kind of defeats the whole purpose. We need a way to approximate the similarities without having to compute these sets. We also need to approximate the similarities for all pairs of users, not just a given pair. In order to do that, we’re going to switch our view of the data from sets to a matrix.</p>

<p><img src="../images/user_matrix.png" width="50%" /></p>

<p>The columns represent the users and the rows represent the songs. A given user has a <script type="math/tex">1</script> in a particular row if they have the song that represents that row in their listening history <sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>.</p>

<h3 id="min-hashing">Min Hashing</h3>

<p>To reiterate the goal, we want to encode the data into a smaller representation such that the Jaccard similarities are preserved. In more “mathy” terms, we have a data matrix <script type="math/tex">D</script> that we want to encode in some smaller matrix <script type="math/tex">\hat{D}</script> called the signature matrix, such that <script type="math/tex">J_{pairwise}(D) \approx \hat{J}_{pairwise}(\hat{D})</script> <sup id="fnref:4"><a href="#fn:4" class="footnote">3</a></sup>.</p>

<p>The first algorithm I will be describing is not really practical but it’s a good way to introduce the actual algorithm called MinHash. The whole procedure can be summarized in a sentence: shuffle the rows of the data matrix and for each user (column) store the ID of the first non-zero element. That’s it!</p>

<p><strong>naive-minhashing</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for k iterations
  shuffle rows
  for each column
    store the ID of first non-zero element into the signature matrix
</code></pre></div></div>
<p>Let’s go through one iteration of this algorithm:</p>

<p><img src="../images/1iteration.png" alt="" /></p>

<p>To compute the Jaccard similarities between the users we do an element-wise comparison of the signatures. Let <script type="math/tex">h</script> be the function that finds and returns the index of the first non-zero element. Then we have:</p>

<center>
$$h(\text{user}_{1}) = 7$$
$$h(\text{user}_{2}) = 0$$
$$h(\text{user}_{3}) = 0$$
</center>

<p>And the Jaccard similarities are estimated as  <sup id="fnref:3"><a href="#fn:3" class="footnote">4</a></sup>:</p>

<script type="math/tex; mode=display">\hat{J}(\text{user}_{i}, \text{user}_{j}) = \unicode{x1D7D9}[h(\text{user}_{i}) = h(\text{user}_{j})]</script>

<p><strong>Why would this work?</strong></p>

<p>The probability that <script type="math/tex">h(\text{user}_{i})</script> and <script type="math/tex">h(\text{user}_{j})</script> are the same is exactly <script type="math/tex">J(\text{user}_{i}, \text{user}_{j})</script>. That is, we’re claiming that <script type="math/tex">P(h(\text{user}_{i}) = h(\text{user}_{j})) = J(\text{user}_{i}, \text{user}_{j})</script>. What this means is that, if a pair of users have a high similarity, there is a high probability they will have the same value for <script type="math/tex">h</script>. Do you remember throwing darts at the diagrams? The intuition is the same here.</p>

<p>To make sense of why this is true, let’s again focus on a particular pair <script type="math/tex">\text{user}_{1}</script> and <script type="math/tex">\text{user}_{2}</script>. I’ve highlighted the relevant rows using the same definition for the symbols “+” and “-“ as before. We have an additional symbol called “null”, these correspond to elements that are in neither of the selected users. The “null” type rows can be ignored since they do not contribute to the similarity.</p>

<p><img src="../images/user_matrix_highlighted.png" width="50%" /></p>

<p>If we shuffled the rows what is the probability that the first <strong>non</strong>-“null” row is of type “+”? In other words, after shuffling the rows, if we proceeded from top to bottom while skipping over all “null” rows, what is the probability of seeing a “+” before seeing a “-“?</p>

<p>If we think back to our example with sets, this question should be easy to answer. All we have to realize is that, encountering a “+” before a “-“ is the exact same thing randomly drawing a “+” in the union. Which we know has a probability that is equal to the Jaccard similarity.</p>

<!-- [^3]: The only difference is that we're using shuffling instead of randomly picking, which we've concluded is the exact same thing. -->

<script type="math/tex; mode=display">P(\text{seeing a "+" before "-"})  = \frac{\text{number of "+"}}{\text{number of "+" and "-"}} = J(\text{user}_{1}, \text{user}_{2})</script>

<p>If the first row is of type “+” that also means that <script type="math/tex">h(\text{user}_{1}) = h(\text{user}_{2})</script>, so the above expression is equivalent to saying:</p>

<script type="math/tex; mode=display">P(h(\text{user}_{1}) = h(\text{user}_{2})) = J(\text{user}_{1}, \text{user}_{2})</script>

<p>The same argument holds for any pair of users.</p>

<!-- Remember, throwing darts at the diagrams? That's exactly what we're doing here. You can think of this process as throwing a dart on the diagram and then checking if it landed in an intersection. -->

<p>Going back to our example, we have pairs (<script type="math/tex">\text{user}_1</script>, <script type="math/tex">\text{user}_2</script>) and (<script type="math/tex">\text{user}_1</script>, <script type="math/tex">\text{user}_3</script>) having similarity zero since their signatures do not match. The similarity for (<script type="math/tex">\text{user}_2</script>, <script type="math/tex">\text{user}_3</script>) will be 1 since both have the same signature.</p>

<p><img src="../images/sig1.png" width="50%" /></p>

<p>As you can see it’s a <em>little</em> off. This is because we’re only using a single signature to measure the similarities. We’ve mentioned before that the more random simulations we run the better the approximation will be. In order to have a better approximation, we should run a few more iterations of this process. This would result in a larger signature matrix.</p>

<p>The animation below shows the process of going through 3 iterations of this algorithm:</p>

<p><img src="../images/permuation_animation.gif" alt="" /></p>

<p>Computing the Jaccard similarities with the larger signature matrix:</p>

<p><img src="../images/sig3_sims.png" /></p>

<p>That’s much better. It’s still not exactly the same but it’s not too far off. We’ve managed to reduce the number of rows of the matrix from 8 to 3 while preserving the pairwise Jaccard similarities up to some error. To achieve a better accuracy, we could construct a larger signature matrix, but obviously we would be trading off the size of the representation.</p>

<p>If you want to play around with this algorithm, here’s an implementation in Python using Numpy:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">min_hashing_naive</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">num_iter</span><span class="p">):</span>
    <span class="n">num_users</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">sig</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="n">num_users</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_users</span><span class="p">):</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="nb">any</span><span class="p">(</span><span class="n">c</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
                <span class="n">min_hash</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">c</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">sig</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_hash</span>
    <span class="k">return</span> <span class="n">sig</span>
</code></pre></div></div>

<h3 id="minhash-algorithm">MinHash Algorithm</h3>

<p>Shuffling the rows of the data matrix can be slow, especially if the matrix is large. We would like to avoid doing this. What we’re going to do instead is <em>implicitly</em> shuffle the rows by generating a permutation on the indices of the rows. In order to do this, we’re going to introduce hash functions.</p>

<p>A hash function <script type="math/tex">h</script> will map every index in the row to some unique integer. It’s not a requirement for the range of the hash values to be the same as the indices, but let’s assume for the sake of this example that it is. Then you can think of these permutations as where the row would have landed if we actually randomly shuffled the rows. For example with 8 rows, the hash function could map them to:</p>

<script type="math/tex; mode=display">[0, 1, 2, 3, 4, 5, 6, 7] \rightarrow [4, 1, 5, 6, 0, 2, 3, 7]</script>

<p>To simulate multiple iterations of shuffling, we’re going to apply multiple hash functions to every index.</p>

<p><strong>Recipe for generating hash functions</strong></p>

<p>Pick a prime number <script type="math/tex">p \ge m</script> where <script type="math/tex">m</script> is the number of rows in the dataset. Then each hash function <script type="math/tex">h_{i}</script> can be defined as:</p>

<script type="math/tex; mode=display">h_{i}(x) = (a_{i}x + b_{i}) \mod p</script>

<p>Where <script type="math/tex">a_{i}, b_{i}</script> are random integers in the range <script type="math/tex">[1, p)</script> and <script type="math/tex">[0, p)</script>, respectively. The input <script type="math/tex">x</script> to the function is the index of the row. To generate a hash function, all we have to do is pick the parameters <script type="math/tex">a</script> and <script type="math/tex">b</script>.</p>

<p>For example, let’s define three hash functions: <script type="math/tex">h_{1}, h_{2}, h_{3}</script></p>

<center>
$$h_{1}(x) = 7x \mod 11$$
$$h_{2}(x) = (x + 5) \mod 11$$
$$h_{3}(x) = (3x + 1) \mod 11$$
</center>

<p>We’ll be applying these hash functions to the rows of our toy dataset. Since the number of rows <script type="math/tex">m = 8</script> is not a prime number we chose <script type="math/tex">p = 11</script> (the closest prime to <script type="math/tex">m</script>). The reason for doing this is because we don’t want to have collisions, that is we don’t want more than one row to map to the same value for a given hash function.</p>

<p>As I’ve mentioned before, the values of the hash function need not be in the same range as the indices. As long as each index is mapped to a unique value, the range of the values actually makes no difference. If this doesn’t make sense to you, let’s unpack the example we have. In this case, our hash functions will produce values in the range <script type="math/tex">[0, 10]</script>. We can imagine expanding our dataset with a bunch of “null” type rows so that we have <script type="math/tex">m=p=11</script> rows. We know that the “null” rows don’t change the probability of two users having the same signature, therefore our estimates should be be unaffected.</p>

<!-- But what this means is that our hash functions will produce values in the range $$[0, 10]$$, which is larger than your set of indices. This will actually end up not making any difference. To see why we can imagine expanding our dataset with a bunch of "null" type rows so that we have $$m=11$$. We know that the "null" rows don't change the probability of two users having the same signature, therefore having a range bigger than the actual is not going to change anything. -->

<p><img src="../images/user_matrix_hash.png" width="50%" /></p>

<p>Now that we have the hash functions, we’re finally ready for the MinHash algorithm:</p>

<p><strong>MinHash</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Initialize all elements in the signature matrix sig to infinity
for each row r in the dataset
  Compute h_{i}(r) for all hash functions h_{1}, h_{2}, ..., h_{k}
  for each non-zero column c in the row r
    if sig(i, c) &gt; h_{i}(r)
      update sig(i, c) = h_{i}(r)
</code></pre></div></div>

<p>When the algorithm terminates the signature matrix should contain all the minimum hash values for each user and hash function pair. The video below is an animation that simulates the algorithm over the toy dataset. Watching it should hopefully clear up any questions you have about how or why the algorithm works.</p>

<center> <iframe width="560" height="315" src="https://www.youtube.com/embed/wV5v153Lj5w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </center>

<hr />

<p>If you have any questions or you see any mistakes, please feel free to use the comment section below.</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>I know shuffling a set of elements is meaningless since sets don’t have order but imagine that they do :). <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Obviously, we wouldn’t store the data in this form in practice, since it’s extremely wasteful. But seeing the data as a matrix will be a helpful for conceptualizing the methods that we’re gonna discuss. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p><script type="math/tex">J_{pairwise}</script> is a function that produces a matrix which represents all pairwise similarities of the users in the data. <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>The strange looking one (<script type="math/tex">\unicode{x1D7D9}</script>) is called the indicator function. It outputs a 1 if the expression inside the brackets evaluates to true, otherwise the output is a 0. <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>

  <div>
    <p><span class="share-box">Share:</span> <a href="http://twitter.com/share?text=Min Hashing&url=http://localhost:4000/minhashing/" target="_blank">Twitter</a>, <a href="https://www.facebook.com/sharer.php?u=http://localhost:4000/minhashing/" target="_blank">Facebook</a></p>
  </div>

  <!--<div class="date">
    Written on March 10, 2020
  </div>-->

  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'giothinks';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>


<script type="text/javascript" async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

<script type="text/javascript" src="../assets/js/jquery-3.4.1.min.js"></script>
<script type="text/javascript" src="../assets/js/bigfoot.min.js"></script>
<script type="text/javascript">
  $.bigfoot({
    activateCallback: function($popover, $button) {
        if (MathJax && !$button.data('mathjax-processed')) {
            var content_wrapper = $popover.find('.bigfoot-footnote__content')[0];
            MathJax.Hub.Queue(['Typeset', MathJax.Hub, content_wrapper]);
            MathJax.Hub.Queue(function () {
                $button.attr('data-bigfoot-footnote', content_wrapper.innerHTML);
                $button.data('mathjax-processed', true);
            });
        }
    }
});
</script>
</article>

        </div>
      </div>

      <div class="wrapper-footer">
        <div class="container">
          <footer class="footer">
            
<a href="mailto:giorgi@cs.utah.edu"><i class="svg-icon email"></i></a>


<a href="https://github.com/colonialjelly"><i class="svg-icon github"></i></a>



<a href="/feed.xml"><i class="svg-icon rss"></i></a>
<a href="https://www.twitter.com/colonialjelly"><i class="svg-icon twitter"></i></a>





          </footer>
        </div>
      </div>
    </div>
    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-157355591-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/minhashing/',
		  'title': 'Min Hashing'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
